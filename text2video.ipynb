{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ddf73f",
   "metadata": {},
   "source": [
    "!pip install diffusers transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fc8e1",
   "metadata": {},
   "source": [
    "!pip install --upgrade diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91bdccd",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6f1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to create a video from a list of frames\n",
    "def create_video(frames, output_path, fps):\n",
    "    # Determine the size of the frames\n",
    "    height, width, _ = frames[0].shape\n",
    "\n",
    "    # Create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for video output (change it as needed)\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write each frame to the video file\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "\n",
    "    print(\"Video created successfully!:>>> \"+output_path)\n",
    "\n",
    "\n",
    "# Path to the directory containing the frames\n",
    "\n",
    "# Load each frame and add it to a list\n",
    "\"\"\"\n",
    "frames = []\n",
    "for frame_file in frame_files:\n",
    "    frame_path = os.path.join(frames_dir, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    frames.append(frame)\n",
    "\"\"\"\n",
    "# Output video path\n",
    "output_video = \"text2video/0208/video.mp4\"\n",
    "\n",
    "# Frames per second (change it as needed)\n",
    "fps = 24\n",
    "\n",
    "# Create the video from the frames\n",
    "#create_video(image.frames, output_video, fps)\n",
    "#create_video(video_frames, output_video, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c86c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on VideoToVideoSDPipeline in module diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_synth_img2img object:\n",
      "\n",
      "class VideoToVideoSDPipeline(diffusers.pipelines.pipeline_utils.DiffusionPipeline, diffusers.loaders.TextualInversionLoaderMixin, diffusers.loaders.LoraLoaderMixin)\n",
      " |  VideoToVideoSDPipeline(vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_3d_condition.UNet3DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers)\n",
      " |  \n",
      " |  Pipeline for text-to-video generation.\n",
      " |  \n",
      " |  This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
      " |  library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
      " |  \n",
      " |  Args:\n",
      " |      vae ([`AutoencoderKL`]):\n",
      " |          Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n",
      " |      text_encoder ([`CLIPTextModel`]):\n",
      " |          Frozen text-encoder. Same as Stable Diffusion 2.\n",
      " |      tokenizer (`CLIPTokenizer`):\n",
      " |          Tokenizer of class\n",
      " |          [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
      " |      unet ([`UNet3DConditionModel`]): Conditional U-Net architecture to denoise the encoded video latents.\n",
      " |      scheduler ([`SchedulerMixin`]):\n",
      " |          A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
      " |          [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VideoToVideoSDPipeline\n",
      " |      diffusers.pipelines.pipeline_utils.DiffusionPipeline\n",
      " |      diffusers.configuration_utils.ConfigMixin\n",
      " |      diffusers.loaders.TextualInversionLoaderMixin\n",
      " |      diffusers.loaders.LoraLoaderMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: Union[str, List[str]] = None, video: Union[List[numpy.ndarray], torch.FloatTensor] = None, strength: float = 0.6, num_inference_steps: int = 50, guidance_scale: float = 15.0, negative_prompt: Union[str, List[str], NoneType] = None, eta: float = 0.0, generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None, latents: Union[torch.FloatTensor, NoneType] = None, prompt_embeds: Union[torch.FloatTensor, NoneType] = None, negative_prompt_embeds: Union[torch.FloatTensor, NoneType] = None, output_type: Union[str, NoneType] = 'np', return_dict: bool = True, callback: Union[Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None, callback_steps: int = 1, cross_attention_kwargs: Union[Dict[str, Any], NoneType] = None)\n",
      " |          Function invoked when calling the pipeline for generation.\n",
      " |      \n",
      " |          Args:\n",
      " |              prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts to guide the video generation. If not defined, one has to pass `prompt_embeds`.\n",
      " |                  instead.\n",
      " |              video: (`List[np.ndarray]` or `torch.FloatTensor`):\n",
      " |                  `video` frames or tensor representing a video batch, that will be used as the starting point for the\n",
      " |                  process. Can also accpet video latents as `image`, if passing latents directly, it will not be encoded\n",
      " |                  again.\n",
      " |              strength (`float`, *optional*, defaults to 0.8):\n",
      " |                  Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1. `image`\n",
      " |                  will be used as a starting point, adding more noise to it the larger the `strength`. The number of\n",
      " |                  denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will\n",
      " |                  be maximum and the denoising process will run for the full number of iterations specified in\n",
      " |                  `num_inference_steps`. A value of 1, therefore, essentially ignores `image`.\n",
      " |              num_inference_steps (`int`, *optional*, defaults to 50):\n",
      " |                  The number of denoising steps. More denoising steps usually lead to a higher quality videos at the\n",
      " |                  expense of slower inference.\n",
      " |              guidance_scale (`float`, *optional*, defaults to 7.5):\n",
      " |                  Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
      " |                  `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
      " |                  Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
      " |                  1`. Higher guidance scale encourages to generate videos that are closely linked to the text `prompt`,\n",
      " |                  usually at the expense of lower video quality.\n",
      " |              negative_prompt (`str` or `List[str]`, *optional*):\n",
      " |                  The prompt or prompts not to guide the video generation. If not defined, one has to pass\n",
      " |                  `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
      " |                  less than `1`).\n",
      " |              eta (`float`, *optional*, defaults to 0.0):\n",
      " |                  Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
      " |                  [`schedulers.DDIMScheduler`], will be ignored for others.\n",
      " |              generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
      " |                  One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
      " |                  to make generation deterministic.\n",
      " |              latents (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for video\n",
      " |                  generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
      " |                  tensor will ge generated by sampling using the supplied random `generator`. Latents should be of shape\n",
      " |                  `(batch_size, num_channel, num_frames, height, width)`.\n",
      " |              prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
      " |                  provided, text embeddings will be generated from `prompt` input argument.\n",
      " |              negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |                  Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
      " |                  weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
      " |                  argument.\n",
      " |              output_type (`str`, *optional*, defaults to `\"np\"`):\n",
      " |                  The output format of the generate video. Choose between `torch.FloatTensor` or `np.array`.\n",
      " |              return_dict (`bool`, *optional*, defaults to `True`):\n",
      " |                  Whether or not to return a [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] instead of a\n",
      " |                  plain tuple.\n",
      " |              callback (`Callable`, *optional*):\n",
      " |                  A function that will be called every `callback_steps` steps during inference. The function will be\n",
      " |                  called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
      " |              callback_steps (`int`, *optional*, defaults to 1):\n",
      " |                  The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
      " |                  called at every step.\n",
      " |              cross_attention_kwargs (`dict`, *optional*):\n",
      " |                  A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
      " |                  `self.processor` in\n",
      " |                  [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
      " |      \n",
      " |      \n",
      " |      Examples:\n",
      " |          ```py\n",
      " |          >>> import torch\n",
      " |          >>> from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
      " |          >>> from diffusers.utils import export_to_video\n",
      " |      \n",
      " |          >>> pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\n",
      " |          >>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
      " |          >>> pipe.to(\"cuda\")\n",
      " |      \n",
      " |          >>> prompt = \"spiderman running in the desert\"\n",
      " |          >>> video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\n",
      " |          >>> # safe low-res video\n",
      " |          >>> video_path = export_to_video(video_frames, output_video_path=\"./video_576_spiderman.mp4\")\n",
      " |      \n",
      " |          >>> # let's offload the text-to-image model\n",
      " |          >>> pipe.to(\"cpu\")\n",
      " |      \n",
      " |          >>> # and load the image-to-image model\n",
      " |          >>> pipe = DiffusionPipeline.from_pretrained(\n",
      " |          ...     \"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16, revision=\"refs/pr/15\"\n",
      " |          ... )\n",
      " |          >>> pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
      " |          >>> pipe.enable_model_cpu_offload()\n",
      " |      \n",
      " |          >>> # The VAE consumes A LOT of memory, let's make sure we run it in sliced mode\n",
      " |          >>> pipe.vae.enable_slicing()\n",
      " |      \n",
      " |          >>> # now let's upscale it\n",
      " |          >>> video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n",
      " |      \n",
      " |          >>> # and denoise it\n",
      " |          >>> video_frames = pipe(prompt, video=video, strength=0.6).frames\n",
      " |          >>> video_path = export_to_video(video_frames, output_video_path=\"./video_1024_spiderman.mp4\")\n",
      " |          >>> video_path\n",
      " |          ```\n",
      " |      \n",
      " |      \n",
      " |          Returns:\n",
      " |              [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] or `tuple`:\n",
      " |              [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
      " |              When returning a tuple, the first element is a list with the generated frames.\n",
      " |  \n",
      " |  __init__(self, vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_3d_condition.UNet3DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_inputs(self, prompt, strength, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.check_inputs\n",
      " |  \n",
      " |  decode_latents(self, latents)\n",
      " |      # Copied from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_synth.TextToVideoSDPipeline.decode_latents\n",
      " |  \n",
      " |  disable_vae_slicing(self)\n",
      " |      Disable sliced VAE decoding. If `enable_vae_slicing` was previously invoked, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  disable_vae_tiling(self)\n",
      " |      Disable tiled VAE decoding. If `enable_vae_tiling` was previously invoked, this method will go back to\n",
      " |      computing decoding in one step.\n",
      " |  \n",
      " |  enable_model_cpu_offload(self, gpu_id=0)\n",
      " |      Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n",
      " |      to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n",
      " |      method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n",
      " |      `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n",
      " |  \n",
      " |  enable_sequential_cpu_offload(self, gpu_id=0)\n",
      " |      Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n",
      " |      text_encoder, vae have their state dicts saved to CPU and then are moved to a `torch.device('meta') and loaded\n",
      " |      to GPU only when their specific submodule has its `forward` method called. Note that offloading happens on a\n",
      " |      submodule basis. Memory savings are higher than with `enable_model_cpu_offload`, but performance is lower.\n",
      " |  \n",
      " |  enable_vae_slicing(self)\n",
      " |      Enable sliced VAE decoding.\n",
      " |      \n",
      " |      When this option is enabled, the VAE will split the input tensor in slices to compute decoding in several\n",
      " |      steps. This is useful to save some memory and allow larger batch sizes.\n",
      " |  \n",
      " |  enable_vae_tiling(self)\n",
      " |      Enable tiled VAE decoding.\n",
      " |      \n",
      " |      When this option is enabled, the VAE will split the input tensor into tiles to compute decoding and encoding in\n",
      " |      several steps. This is useful to save a large amount of memory and to allow the processing of larger images.\n",
      " |  \n",
      " |  get_timesteps(self, num_inference_steps, strength, device)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline.get_timesteps\n",
      " |  \n",
      " |  prepare_extra_step_kwargs(self, generator, eta)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n",
      " |  \n",
      " |  prepare_latents(self, video, timestep, batch_size, dtype, device, generator=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Any)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  disable_attention_slicing(self)\n",
      " |      Disable sliced attention computation. If `enable_attention_slicing` was previously called, attention is\n",
      " |      computed in one step.\n",
      " |  \n",
      " |  disable_xformers_memory_efficient_attention(self)\n",
      " |      Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n",
      " |  \n",
      " |  enable_attention_slicing(self, slice_size: Union[str, int, NoneType] = 'auto')\n",
      " |      Enable sliced attention computation.\n",
      " |      \n",
      " |      When this option is enabled, the attention module splits the input tensor in slices to compute attention in\n",
      " |      several steps. This is useful to save some memory in exchange for a small speed decrease.\n",
      " |      \n",
      " |      Args:\n",
      " |          slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
      " |              When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
      " |              `\"max\"`, maximum amount of memory will be saved by running only one slice at a time. If a number is\n",
      " |              provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n",
      " |              must be a multiple of `slice_size`.\n",
      " |  \n",
      " |  enable_xformers_memory_efficient_attention(self, attention_op: Union[Callable, NoneType] = None)\n",
      " |      Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n",
      " |      \n",
      " |      When this option is enabled, you should observe lower GPU memory usage and a potential speed up during\n",
      " |      inference. Speed up during training is not guaranteed.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      â ï¸ When memory efficient attention and sliced attention are both enabled, memory efficient attention takes\n",
      " |      precedent.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          attention_op (`Callable`, *optional*):\n",
      " |              Override the default `None` operator for use as `op` argument to the\n",
      " |              [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n",
      " |              function of xFormers.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
      " |      \n",
      " |      >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
      " |      >>> pipe = pipe.to(\"cuda\")\n",
      " |      >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
      " |      >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n",
      " |      >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
      " |      ```\n",
      " |  \n",
      " |  progress_bar(self, iterable=None, total=None)\n",
      " |  \n",
      " |  register_modules(self, **kwargs)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = False, variant: Union[str, NoneType] = None)\n",
      " |      Save all saveable variables of the pipeline to a directory. A pipeline variable can be saved and loaded if its\n",
      " |      class implements both a save and loading method. The pipeline is easily reloaded using the\n",
      " |      [`~DiffusionPipeline.from_pretrained`] class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save a pipeline to. Will be created if it doesn't exist.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
      " |  \n",
      " |  set_attention_slice(self, slice_size: Union[int, NoneType])\n",
      " |  \n",
      " |  set_progress_bar_config(self, **kwargs)\n",
      " |  \n",
      " |  set_use_memory_efficient_attention_xformers(self, valid: bool, attention_op: Union[Callable, NoneType] = None) -> None\n",
      " |  \n",
      " |  to(self, torch_device: Union[str, torch.device, NoneType] = None, torch_dtype: Union[torch.dtype, NoneType] = None, silence_dtype_warnings: bool = False)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  download(pretrained_model_name, **kwargs) -> Union[str, os.PathLike] from builtins.type\n",
      " |      Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name (`str` or `os.PathLike`, *optional*):\n",
      " |              A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |              hosted on the Hub.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called `pipeline.py` that defines\n",
      " |                    the custom pipeline.\n",
      " |      \n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current `main` branch of GitHub.\n",
      " |      \n",
      " |                  - A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              ð§ª This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, take a look at [How to contribute a\n",
      " |              community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a ð¤ Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `os.PathLike`:\n",
      " |              A path to the downloaded pipeline.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) from builtins.type\n",
      " |      Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
      " |      \n",
      " |      If you get the error message below, you need to finetune the weights for your downstream task:\n",
      " |      \n",
      " |      ```\n",
      " |      Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      " |      - conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
      " |      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " |      ```\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |                    hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing pipeline weights\n",
      " |                    saved using\n",
      " |                  [`~DiffusionPipeline.save_pretrained`].\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model with another dtype. If \"auto\" is passed, the\n",
      " |              dtype is automatically derived from the model's weights.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              ð§ª This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`) of a custom\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called pipeline.py that defines\n",
      " |                    the custom pipeline.\n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current main branch of GitHub.\n",
      " |                  - A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
      " |              Adding Custom\n",
      " |              Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a ð¤ Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if youâre downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      " |              A map that specifies where each submodule should go. It doesnât need to be defined for each\n",
      " |              parameter/buffer name; once a given module name is inside, every submodule of it will be sent to the\n",
      " |              same device.\n",
      " |      \n",
      " |              Set `device_map=\"auto\"` to have ð¤ Accelerate automatically compute the most optimized `device_map`. For\n",
      " |              more information about each option see [designing a device\n",
      " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      " |          max_memory (`Dict`, *optional*):\n",
      " |              A dictionary device identifier for the maximum memory. Will default to the maximum memory available for\n",
      " |              each GPU and the available CPU RAM if unset.\n",
      " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
      " |              The path to offload weights if device_map contains the value `\"disk\"`.\n",
      " |          offload_state_dict (`bool`, *optional*):\n",
      " |              If `True`, temporarily offloads the CPU state dict to the hard drive to avoid running out of CPU RAM if\n",
      " |              the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to `True`\n",
      " |              when there is some disk offload.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      " |              weights. If set to `False`, safetensors weights are not loaded.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
      " |              class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
      " |              below for more information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models) models, log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      \n",
      " |      >>> # Download pipeline from huggingface.co and cache.\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
      " |      \n",
      " |      >>> # Download pipeline that requires an authorization token\n",
      " |      >>> # For more information on access tokens, please refer to this section\n",
      " |      >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      \n",
      " |      >>> # Use a different scheduler\n",
      " |      >>> from diffusers import LMSDiscreteScheduler\n",
      " |      \n",
      " |      >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
      " |      >>> pipeline.scheduler = scheduler\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  numpy_to_pil(images)\n",
      " |      Convert a NumPy image or a batch of images to a PIL image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  components\n",
      " |      The `self.components` property can be useful to run different pipelines with the same weights and\n",
      " |      configurations without reallocating additional memory.\n",
      " |      \n",
      " |      Returns (`dict`):\n",
      " |          A dictionary containing all the modules needed to initialize the pipeline.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import (\n",
      " |      ...     StableDiffusionPipeline,\n",
      " |      ...     StableDiffusionImg2ImgPipeline,\n",
      " |      ...     StableDiffusionInpaintPipeline,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> text2img = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
      " |      >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n",
      " |      ```\n",
      " |  \n",
      " |  device\n",
      " |      Returns:\n",
      " |          `torch.device`: The torch device on which the pipeline is located.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  config_name = 'model_index.json'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      The only reason we overwrite `getattr` here is to gracefully deprecate accessing\n",
      " |      config attributes directly. See https://github.com/huggingface/diffusers/pull/3129\n",
      " |      \n",
      " |      Tihs funtion is mostly copied from PyTorch's __getattr__ overwrite:\n",
      " |      https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  register_to_config(self, **kwargs)\n",
      " |  \n",
      " |  save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      " |      Save a configuration object to the directory specified in `save_directory` so that it can be reloaded using the\n",
      " |      [`~ConfigMixin.from_config`] class method.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the configuration JSON file is saved (will be created if it does not exist).\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save the configuration instance's parameters to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file to save a configuration instance's parameters.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes the configuration instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`:\n",
      " |              String containing all the attributes that make up the configuration instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  extract_init_dict(config_dict, **kwargs) from builtins.type\n",
      " |  \n",
      " |  from_config(config: Union[diffusers.configuration_utils.FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs) from builtins.type\n",
      " |      Instantiate a Python class from a config dictionary.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          config (`Dict[str, Any]`):\n",
      " |              A config dictionary from which the Python class is instantiated. Make sure to only load configuration\n",
      " |              files of compatible classes.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether kwargs that are not consumed by the Python class should be returned or not.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it is loaded) and initiate the Python class.\n",
      " |              `**kwargs` are passed directly to the underlying scheduler/model's `__init__` method and eventually\n",
      " |              overwrite the same named arguments in `config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`ModelMixin`] or [`SchedulerMixin`]:\n",
      " |              A model or scheduler object instantiated from a config dictionary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n",
      " |      \n",
      " |      >>> # Download scheduler from huggingface.co and cache.\n",
      " |      >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n",
      " |      \n",
      " |      >>> # Instantiate DDIM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n",
      " |      \n",
      " |      >>> # Instantiate PNDM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n",
      " |      ```\n",
      " |  \n",
      " |  get_config_dict(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  load_config(pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, return_commit_hash=False, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      Load a model or scheduler configuration.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with\n",
      " |                    [`~ConfigMixin.save_config`].\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False):\n",
      " |              Whether unused keyword arguments of the config are returned.\n",
      " |          return_commit_hash (`bool`, *optional*, defaults to `False):\n",
      " |              Whether the `commit_hash` of the loaded configuration are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict`:\n",
      " |              A dictionary of all the parameters stored in a JSON configuration file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  config\n",
      " |      Returns the config of the class as a frozen dictionary\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Config of the class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  has_compatibles = False\n",
      " |  \n",
      " |  ignore_for_config = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.TextualInversionLoaderMixin:\n",
      " |  \n",
      " |  load_textual_inversion(self, pretrained_model_name_or_path: Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]], token: Union[str, List[str], NoneType] = None, **kwargs)\n",
      " |      Load textual inversion embeddings into the text encoder of [`StableDiffusionPipeline`] (both ð¤ Diffusers and\n",
      " |      Automatic1111 formats are supported).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike` or `List[str or os.PathLike]` or `Dict` or `List[Dict]`):\n",
      " |              Can be either one of the following or a list of them:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`) of a\n",
      " |                    pretrained model hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_text_inversion_directory/`) containing the textual\n",
      " |                    inversion weights.\n",
      " |                  - A path to a *file* (for example `./my_text_inversions.pt`) containing textual inversion weights.\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          token (`str` or `List[str]`, *optional*):\n",
      " |              Override the token to use for the textual inversion weights. If `pretrained_model_name_or_path` is a\n",
      " |              list, then `token` must also be a list of equal length.\n",
      " |          weight_name (`str`, *optional*):\n",
      " |              Name of a custom weight file. This should be used when:\n",
      " |      \n",
      " |                  - The saved textual inversion file is in ð¤ Diffusers format, but was saved under a specific weight\n",
      " |                    name such as `text_inv.bin`.\n",
      " |                  - The saved textual inversion file is in the Automatic1111 format.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in ð¤ Diffusers format:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
      " |      \n",
      " |      prompt = \"A <cat-toy> backpack\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"cat-backpack.png\")\n",
      " |      ```\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in Automatic1111 format, make sure to download the vector first\n",
      " |      (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857)) and then load the vector\n",
      " |      locally:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n",
      " |      \n",
      " |      prompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"character.png\")\n",
      " |      ```\n",
      " |  \n",
      " |  maybe_convert_prompt(self, prompt: Union[str, List[str]], tokenizer: 'PreTrainedTokenizer')\n",
      " |      Processes prompts that include a special token corresponding to a multi-vector textual inversion embedding to\n",
      " |      be replaced with multiple special tokens each corresponding to one of the vectors. If the prompt has no textual\n",
      " |      inversion token or if the textual inversion token is a single vector, the input prompt is returned.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prompt (`str` or list of `str`):\n",
      " |              The prompt or prompts to guide the image generation.\n",
      " |          tokenizer (`PreTrainedTokenizer`):\n",
      " |              The tokenizer responsible for encoding the prompt into input tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or list of `str`: The converted prompt\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  load_lora_weights(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs)\n",
      " |      Load pretrained LoRA attention processor layers into [`UNet2DConditionModel`] and\n",
      " |      [`CLIPTextModel`](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved\n",
      " |                    with [`ModelMixin.save_pretrained`].\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  save_lora_weights(save_directory: Union[str, os.PathLike], unet_lora_layers: Dict[str, Union[torch.nn.modules.module.Module, torch.Tensor]] = None, text_encoder_lora_layers: Dict[str, torch.nn.modules.module.Module] = None, is_main_process: bool = True, weight_name: str = None, save_function: Callable = None, safe_serialization: bool = False) from builtins.type\n",
      " |      Save the LoRA parameters corresponding to the UNet and text encoder.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save LoRA parameters to. Will be created if it doesn't exist.\n",
      " |          unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the UNet.\n",
      " |          text_encoder_lora_layers (`Dict[str, torch.nn.Module] or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text\n",
      " |              encoder LoRA state dict because it comes ð¤ Transformers.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful during distributed training and you\n",
      " |              need to call this function on all processes. In this case, set `is_main_process=True` only on the main\n",
      " |              process to avoid race conditions.\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful during distributed training when you need to\n",
      " |              replace `torch.save` with another method. Can be configured with the environment variable\n",
      " |              `DIFFUSERS_SAVE_MODE`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  lora_scale\n",
      " |  \n",
      " |  text_encoder_lora_attn_procs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  text_encoder_name = 'text_encoder'\n",
      " |  \n",
      " |  unet_name = 'unet'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cbb019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff05e547280a4cc2ae8b966c09fba322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to save in text2video/a_macro_video_of_doves_flying,_taking_off,_award_winning,_8k,_national_geographic.mp4\n",
      "Video created successfully!:>>> text2video/a_macro_video_of_doves_flying,_taking_off,_award_winning,_8k,_national_geographic.mp4\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "#pipe.enable_model_cpu_offload()\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "#prompt = \"a computer becomes biological, showing the anatomy of human nervous system\"\n",
    "#prompt = \"intense techno rave as it twists its cyclical time rave intense dance\"\n",
    "#prompt = \"cyclical human nervous system is a jigsaw puzzle, by Ernst Haeckel\"\n",
    "#prompt = \"CLOSE UP of dead doves flocking on the sidewalk, renaissance oil painting, an ultrafine detailed painting, Cimabue, neo-figurative\"\n",
    "prompt = \"a macro video of doves flying, taking off, award winning, 8k, national geographic\"\n",
    "negative_prompt = \"noisy, washed out, distorted, broken, ugly\"\n",
    "guidance_scale = 7.5\n",
    "video_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24, guidance_scale=guidance_scale, negative_prompt=negative_prompt).frames\n",
    "#video_path = export_to_video(video_frames)\n",
    "output_video = \"text2video/\"+prompt.replace(\" \",\"_\")+\"_2.mp4\"\n",
    "print(\"about to save in \"+output_video)\n",
    "\n",
    "create_video(video_frames, output_video, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba3d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully!:>>> text2video/CYCLICAL_doves_flying,_taking_off,_city,_by_Ernst_Haeckel.mp4\n"
     ]
    }
   ],
   "source": [
    "output_video = \"text2video/\"+prompt.replace(\" \",\"_\")+\".mp4\"\n",
    "\n",
    "create_video(video_frames, output_video, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c0ddbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_computer_becomes_biological,_showing_the_anatomy_of_human_nervous_system_1024.mp4\r\n",
      "a_computer_becomes_biological,_showing_the_anatomy_of_human_nervous_system.mp4\r\n",
      "a_dead_dove_on_the_sidewalk,_renaissance_oil_painting,_an_ultrafine_detailed_painting,_Cimabue,_neo-figurative_1024.mp4\r\n",
      "a_dead_dove_on_the_sidewalk,_renaissance_oil_painting,_an_ultrafine_detailed_painting,_Cimabue,_neo-figurative.mp4\r\n",
      "a_machine_is_a_colorful_anatomical_jigsaw_puzzle_of_human_parts,_veins,_arteries,_intestines,_Ernst_Haeckel_1024.mp4\r\n",
      "a_machine_is_a_colorful_anatomical_jigsaw_puzzle_of_human_parts,_veins,_arteries,_intestines,_Ernst_Haeckel.mp4\r\n",
      "a_machine_is_an_anatomical_jigsaw_puzzle_of_human_parts,_veins,_arteries,_intestines,_Ernst_Haeckel_1024.mp4\r\n",
      "a_machine_is_an_anatomical_jigsaw_puzzle_of_human_parts,_veins,_arteries,_intestines,_Ernst_Haeckel.mp4\r\n",
      "a_tray_of_red_and_white_veins_attached_to_them,_anatomical,_a_jigsaw_puzzle,_Ernst_Haeckel,_academic_art..mp4\r\n",
      "by_Ernst_Haeckel_1024.mp4\r\n",
      "by_Ernst_Haeckel.mp4\r\n",
      "colorful_cyclical_doves_flying,_by_Ernst_Haeckel_1024.mp4\r\n",
      "colorful_CYCLICAL_doves_flying,_by_Ernst_Haeckel_1024.mp4\r\n",
      "colorful_cyclical_doves_flying,_by_Ernst_Haeckel.mp4\r\n",
      "colorful_CYCLICAL_doves_flying,_by_Ernst_Haeckel.mp4\r\n",
      "colorful,_human_nervous_system,_a_jigsaw_puzzle,_Ernst_Haeckel_1024.mp4\r\n",
      "colorful,_human_nervous_system,_a_jigsaw_puzzle,_Ernst_Haeckel.mp4\r\n",
      "CYCLICAL_colorful_CYCLICAL_doves_flying,_by_Ernst_Haeckel_1024_2.mp4\r\n",
      "CYCLICAL_colorful_CYCLICAL_doves_flying,_by_Ernst_Haeckel_2.mp4\r\n",
      "CYCLICAL_doves_flying,_by_Ernst_Haeckel_1024_2.mp4\r\n",
      "cyclical_doves_flying,_by_Ernst_Haeckel_1024.mp4\r\n",
      "CYCLICAL_doves_flying,_by_Ernst_Haeckel_2.mp4\r\n",
      "cyclical_doves_flying,_by_Ernst_Haeckel.mp4\r\n",
      "cyclical_human_nervous_system_is_a_jigsaw_puzzle,_by_Ernst_Haeckel_1024.mp4\r\n",
      "cyclical_human_nervous_system_is_a_jigsaw_puzzle,_by_Ernst_Haeckel_2.mp4\r\n",
      "cyclical_human_nervous_system_is_a_jigsaw_puzzle,_by_Ernst_Haeckel.mp4\r\n",
      "intense_techno_rave_as_it_twists_its_cyclical_time_rave_intense_dance_1024.mp4\r\n",
      "intense_techno_rave_as_it_twists_its_cyclical_time_rave_intense_dance.mp4\r\n",
      "red_veins_attached_to_them,_anatomical,_a_jigsaw_puzzle,_Ernst_Haeckel,_academic_art._1024.mp4\r\n",
      "red_veins_attached_to_them,_anatomical,_a_jigsaw_puzzle,_Ernst_Haeckel,_academic_art..mp4\r\n",
      "texture_of_skin_like_flesh_like_being_torn_by_force,_by_Ernst_Haeckel_1024.mp4\r\n",
      "texture_of_skin_like_flesh_like_being_torn_by_force,_by_Ernst_Haeckel.mp4\r\n",
      "very_close_up_of_doves_FLYING_going_crazy.mp4\r\n",
      "very_close_up_of_flocking_of_doves_going_crazy,_Cimabue,_neo-figurative_1024.mp4\r\n",
      "very_close_up_of_flocking_of_doves_going_crazy,_Cimabue,_neo-figurative.mp4\r\n",
      "very_close_up_of_flocking_of_doves_going_crazy,_doves_flying_up_close_1024.mp4\r\n",
      "very_close_up_of_flocking_of_doves_going_crazy,_doves_flying_up_close.mp4\r\n",
      "video2.mp4\r\n",
      "video3.mp4\r\n",
      "video4.mp4\r\n",
      "video5.mp4\r\n",
      "video6.mp4\r\n",
      "video7_small.mp4\r\n",
      "video.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!ls text2video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "151d2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to save in text2video/colorful_cyclical_doves_flying,_by_Ernst_Haeckel.mp4\n",
      "Video created successfully!:>>> text2video/colorful_cyclical_doves_flying,_by_Ernst_Haeckel.mp4\n"
     ]
    }
   ],
   "source": [
    "output_video = \"text2video/\"+prompt.replace(\" \",\"_\")+\"_2.mp4\"\n",
    "print(\"about to save in \"+output_video)\n",
    "\n",
    "create_video(video_frames, output_video, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c998ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b57071feade4c46b9a4353d792278d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "#pipe.enable_model_cpu_offload()\n",
    "pipe.enable_vae_slicing()\n",
    "video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "video_frames = pipe(prompt, video=video, strength=0.6).frames\n",
    "output_video = \"text2video/\"+prompt.replace(\" \",\"_\")+\"_1024.mp4\"\n",
    "print(\"about to save in \"+output_video)\n",
    "\n",
    "create_video(video_frames, output_video, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e675eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to save in text2video/colorful_cyclical_doves_flying,_by_Ernst_Haeckel_1024.mp4\n",
      "Video created successfully!:>>> text2video/colorful_cyclical_doves_flying,_by_Ernst_Haeckel_1024.mp4\n"
     ]
    }
   ],
   "source": [
    "output_video = \"text2video/\"+prompt.replace(\" \",\"_\")+\"_1024.mp4\"\n",
    "print(\"about to save in \"+output_video)\n",
    "\n",
    "create_video(video_frames, output_video, fps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abdf98",
   "metadata": {},
   "source": [
    "# Upscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_XL\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "video = [Image.fromarray(frame).resize((1024, 576)) for frame in video_frames]\n",
    "\n",
    "video_frames = pipe(prompt, video=video, strength=0.6).frames\n",
    "video_path = export_to_video(video_frames, output_video_path=\"/home/patrick/videos/video_1024_darth_vader_36.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
